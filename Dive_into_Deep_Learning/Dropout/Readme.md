# Dropout, 丢弃法
##解决什么问题（What）
很大的神经网络在小数据集上训练，往往会导致过拟合
对每个训练样本，采用dropout的方法，随机删除一半的神经元，可以有效减少过拟合
模型结合通常能够提高机器学习方法的表现。但是，训练大网络的计算代价太高，又要训练一些不同的大网络，代价就更高了，而且还需要大量的数据，现实中往往没有足够的数据。就算训练了那么多大网络，用来实际应用也是不可行的，因为模型太大导致了很慢的响应速度。
可以使用dropout来防止过拟合，它起到了一个（和结合多个不同网络）近似的效果

## 为什么能解决（Why）
因为dropout防止了复杂的共适应co-adaptation，co-adaptation的意思是每个神经元学到的特征，必须结合其它的特定神经元作为上下文，才能提供对训练的帮助。
减小co-adaptation，就是要使每个神经元学到的特征，能更通用地提供帮助，它必须组合大量的内部上下文信息。

## 具体做法（How）
每次更新参数之前，每个神经元有一定的概率被丢弃，假设为p%，p可以设为50或者根据验证集的表现来选取，输入层的p比较小，保留概率接近于1
测试阶段不dropout，保留所有单元的权重，而且要乘以保留概率1-p%，为了保证输出期望一致
dropout不只用于前馈神经网络，还可以用于图模型，比如玻尔兹曼机。

## 对dropout的直观解释（Why）
对每个样本都随机地丢弃一半的单元，使得每个单元不能依赖其它单元来做出决策（这样学到的特征更独立）
训练阶段对于每个mini-batch网络的结构是不一样的（因为随机丢弃了一半的单元），测试阶段每个单元乘以保留概率。这样的效果近似于对多个不同的瘦网络做平均（类似集成的效果，能提供更准确的预测）
可以视作一种正则方法，通过给隐藏单元增加噪音