# Linear Regression 线性回归
给定数据集D={(x1, y1), (x2, y2), ... }，我们试图从此数据集中学习得到一个线性模型，这个模型尽可能准确地反应x(i)和y(i)的对应关系。这里的线性模型，就是属性(x)的线性组合的函数，可表示为：
f(x) = w_1x_1+w_2x_2+...+w_nx_n+b
向量表示为f(x) = W.T.dot(X) + b
这里w表示weight，权重的意思，表示对应的属性在预测结果的权重，这个很好理解，权重越大，对于结果的影响越大；更一般化的表示是theta（Andrew ng的课程就是用theta），是线性模型的参数，用于计算结果。

那么通常的线性回归，就变成了如何求得变量参数的问题，根据求得的参数，我们可以对新的输入来计算预测的值。（也可以用于对训练数据计算模型的准确度）

通俗的理解：x(i)就是一个个属性（例如西瓜书中的色泽，根蒂；Andrew ng示例中的房屋面积，卧室数量等），theta(或者w/b)，就是对应属性的参数（或者权重），我们根据已有数据集来求得属性的参数（相当于求得函数的参数），然后根据模型来对于新的输入或者旧的输入来进行预测（或者评估）。

