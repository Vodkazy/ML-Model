# Recurrent Neural Network, RNN
RNN(Recurrent Neural Network)即循环神经网络，用于解决训练样本输入是连续的序列,且序列的长短不一的问题，比如基于时间序列的问题。基础的神经网络只在层与层之间建立了权连接，RNN最大的不同之处就是在层之间的神经元之间也建立的权连接。

RNN之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。

当前时间步t的隐藏状态H_t将参与下一个时间步t+1的隐藏状态H_t+1的计算，并输入到当前时间步的全连接输出层。

RNN主要用来解决序列问题，强调的是先后顺序，在NLP中引申出上下文的概念，一个翻译问题，这个词的含义可能和前后的单词形成的这个组合有联系（Skip-gram）,也可能是它之前的所有单词都有联系（Attention），并且，借助RNN的state这样的记忆单元，使得一个序列位置的输出在数学上和之前的所有序列的输入都是有关系的。当然原始的RNN由于梯度的乘性问题，前面的序列的影响近乎为0，这个后面又用LSTM来修正为加性问题。RNN的数学基础可以认为是马尔科夫链，认为后续的值是有前者和一些参数的概率决定的。

RNN优点：可以拟合序列数据，（LSTM）通过遗忘门和输出门忘记部分信息来解决梯度消失的问题。缺点：梯度消失；无法很好的并行（工业上影响很大）

鉴于单向循环神经网络某些情况下的不足，提出了双向循环神经网络。因为是需要能关联未来的数据，而单向循环神经网络属于关联历史数据，所以对于未来数据提出了反向循环神经网络，两个方向的网络结合到一起就能关联历史与未来了。
RNN和LSTM都只能依据之前时刻的时序信息来预测下一时刻的输出，但在有些问题中，当前时刻的输出不仅和之前的状态有关，还可能和未来的状态有关系。比如预测一句话中缺失的单词不仅需要根据前文来判断，还需要考虑它后面的内容，真正做到基于上下文判断。BRNN有两个RNN上下叠加在一起组成的，输出由这两个RNN的状态共同决定。
对于每个时刻t，输入会同时提供给两个方向相反的RNN，输出由这两个单向RNN共同决定。
```
前向传播 
    1. 沿着时刻1到时刻T正向计算一遍，得到并保存每个时刻向前隐含层的输出。 
    2. 沿着时刻T到时刻1反向计算一遍，得到并保存每个时刻向后隐含层的输出。 
    3. 正向和反向都计算完所有输入时刻后，每个时刻根据向前向后隐含层得到最终输出。
    
反向传播 
    1. 计算所有时刻输出层的δδ项。 
    2. 根据所有输出层的δδ项，使用 BPTT 算法更新向前层。 
    3. 根据所有输出层的δδ项，使用 BPTT 算法更新向后层。
```

[参考文章](https://www.jianshu.com/p/53e457937557)